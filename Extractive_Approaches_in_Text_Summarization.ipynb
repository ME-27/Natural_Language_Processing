{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccf36b6",
   "metadata": {},
   "source": [
    "# Extractive Approaches in Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d98aa5",
   "metadata": {},
   "source": [
    "https://www.topcoder.com/thrive/articles/text-summarization-in-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69b4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1df3b3b",
   "metadata": {},
   "source": [
    "### TEXT SUMMARIZATION USING THE FREQUENCY METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0a163eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code may conain mistakes!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def solve(text):\n",
    "    stopwords1 = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    freqTable = {}\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopwords1:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else :\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentenceValue = {}\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "            else :\n",
    "                sentenceValue[sentence] = freq\n",
    "                \n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "        average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
    "            summary += \"\" + sentence\n",
    "            \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "14949444",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"my name is shubham kumar shukla. It is my pleasure to get opportunity to write article for xyz related to nlp. This is important!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea46672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is important!'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401ac50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7954e3b3",
   "metadata": {},
   "source": [
    "# sumy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df9c66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48eb74",
   "metadata": {},
   "source": [
    "### LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "36bac2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "\n",
    "def lex_rank_method(text, sentences_count):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    \n",
    "    # Summarize the document with 2 sentences\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)\n",
    "    dp = []\n",
    "    for i in summary:\n",
    "        lp = str(i)\n",
    "        dp.append(lp)\n",
    "        \n",
    "    final_sentence = ' '.join(dp)\n",
    "    \n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c1dba3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is shubham kumar shukla.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_rank_method(data, sentences_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c33f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "594e2b96",
   "metadata": {},
   "source": [
    "### TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce853d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "\n",
    "def text_rank_method(text, sentences_count):\n",
    "    # Creating text parser using tokenization\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "\n",
    "    # Summarize using sumy TextRank\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)\n",
    "\n",
    "    text_summary = \"\"\n",
    "    for sentence in summary:\n",
    "        text_summary += str(sentence)\n",
    "        \n",
    "    return text_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1c101941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is my pleasure to get opportunity to write article for xyz related to nlp.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rank_method(data, sentences_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169aa066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a85997c9",
   "metadata": {},
   "source": [
    "### LuhnSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0a6ac9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "\n",
    "def lunh_method(text, sentences_count):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer_luhn = LuhnSummarizer()\n",
    "    summary_1 = summarizer_luhn(parser.document, sentences_count=sentences_count)\n",
    "    \n",
    "    dp = []\n",
    "    for i in summary_1:\n",
    "        lp = str(i)\n",
    "        dp.append(lp)\n",
    "        \n",
    "    final_sentence = ' '.join(dp)\n",
    "    \n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a8ae92bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is my pleasure to get opportunity to write article for xyz related to nlp.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunh_method(data, sentences_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4741cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea3e8bb",
   "metadata": {},
   "source": [
    "### LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c461f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "\n",
    "def lsa_method(text, sentences_count):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer_lsa = LsaSummarizer()\n",
    "    summary_2 = summarizer_lsa(parser.document, sentences_count=sentences_count)\n",
    "    \n",
    "    dp = []\n",
    "    for i in summary_2:\n",
    "        lp = str(i)\n",
    "        dp.append(lp)\n",
    "        final_sentence = ' '.join(dp)\n",
    "        \n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3df2098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is shubham kumar shukla.'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_method(data, sentences_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46e69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
